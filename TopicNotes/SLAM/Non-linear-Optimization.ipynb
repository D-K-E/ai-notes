{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370eedc1-9873-445b-8425-e486068625cf",
   "metadata": {},
   "source": [
    "## Non linear Optimization\n",
    "\n",
    "State estimation in SLAM depends on the following two equations:\n",
    "\n",
    "- $x_k = f(x_{k-1}, u_k) + w_k$\n",
    "- $z_{k,j} = h(y_j, x_k) + v_{k,j}$\n",
    "\n",
    "The $x_k$ is the camera pose in 6d, $z_{k,j}$ is the image position of the observation $y_j$.\n",
    "The $u_k$ is the input data at time $k$. \n",
    "The input data can be a set of 2d points that are known to belong to the same object in a sequence of images.\n",
    "It can be a mixed set of 3D and 2D points, or a set of 3D points (see Camera-Motion notebook for the related discussion).  \n",
    "\n",
    "The $x_k$ can be expressed as $R_k y_j + t_k$. \n",
    "Under this expression the $x_k$ and $z_{k,j}$ is bound with the following equation:\n",
    "$$s z_{k,j} = K(R_k y_j + t_k) = KT y_j$$\n",
    "where $T \\in SE3D$, $s$ represents the distance of pixels, and $K$ represents the intrinsic camera matrix.\n",
    "\n",
    "The $w_k$ and $v_{k,j}$ are noise terms. They are usually assume to be gaussian with 0 mean.\n",
    "\n",
    "Given the presence of noise, our problem can be formulated as a conditional probability distribution like $P(x, y | z, u)$ meaning that given the pixel position $z$ and input $u$ what is the probability of pose being $x$ and observation being $y$. \n",
    "\n",
    "The values that maximize this probability distribution minimize the error and noise in the system.\n",
    "From the bayes rule, $argmax P(x, y | z, u) = argmax P(z, u | x, y) * P(x, y)$, since we may not know the prior $P(x,y)$ we can also ignore that and transform the Maximizing posterior distribution problem to Maximium likelihood estimation problem, and end up with $$argmax P(x, y| z, u) = argmax P(z, u | x, y)$$.\n",
    "\n",
    "This means that we need to minimize the following terms:\n",
    "- $e_{u, k} = x_k - f(x_{k-1}, u_k)$\n",
    "- $e_{z, j, k} = z_{k, j} - h(x_k, y_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea10d34e-57b7-4e54-81a3-b824de65a13c",
   "metadata": {},
   "source": [
    "## Non linear least-square\n",
    "\n",
    "The term non linear means that our function do not have a linear form like a straight line or a parabola. In the linear case the derivative is constant.\n",
    "In the non linear case, like in exponential function or logarithmic function, the derivative is not constant.\n",
    "\n",
    "Let there be a $m$ set of points and observations such that $(x_1, y_1), (x_2, y_2), \\dots, (x_m, y_m)$\n",
    "Let $h$ be the hypothesis function that binds $x_i$ to $y_i$ using the parameter vector $\\alpha$ such that $$y_i = h(x_i, \\alpha)$$ where $\\alpha \\in \\mathbb{R}^n$\n",
    "\n",
    "Let $r$ be the residual function, that measures the difference between the hypothesis function and the observation $$r(x_i, y_i,\\alpha) = y_i - h(x_i, \\alpha)$$\n",
    "\n",
    "Let $F$ be the objective function that measures the total error resulting from the parameters through summation of squared residuals:\n",
    "$$F(x, y, \\alpha) = \\sum_{i=1}^m r(x_i, y_i, \\alpha)^2$$\n",
    "\n",
    "Since we can't change the data and the observation, our ultimiate goal is to find the set of parameters, $\\alpha$, that minimizes the objective function. The minimum/maximum value of the objective function comes from $\\frac{\\partial F(x, y, \\alpha)}{\\alpha} = 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc662bdd-59e3-4f3d-a3f4-5e805c70994f",
   "metadata": {},
   "source": [
    "Now we know that minimum of a function that sums bunch of values is greater or equal to minimum of functions that is being summed.\n",
    "\n",
    "Formally, $\\forall p, f(p) \\ge min(f(p))$ then $\\forall p, f(p) + f(p) \\ge min(f(p)) + min(f(p))$ and consequently $\\forall p, min(f(p) + f(p)) \\ge min(f(p) + f(p))$\n",
    "\n",
    "Consequently the minimum of $min(F(x, y, \\alpha)) \\ge \\sum_{i=1}^m min(r(x_i, y_i, \\alpha)^2)$. \n",
    "Hence, we can get away with only finding the minimum of $r(\\dots)^2$.\n",
    "\n",
    "Let's take a closer look at $r$ then."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b161565-25dc-4e87-9415-5773e9c55567",
   "metadata": {},
   "source": [
    "Here just like the objective function $F$, we'll use the derivative based on $\\alpha$ as in $\\frac{\\partial k(\\alpha)}{\\alpha}$ where $k(\\alpha) = r(x_i, y_i, \\alpha)^2$ and $k(\\alpha) \\in \\mathbb{R}$\n",
    "\n",
    "Let $\\alpha = \\alpha^s + \\nabla \\alpha$ where $\\alpha^s$ the state of a value provided to $\\alpha$ during the beginning of approximation at the $s$th iteration and $\\nabla \\alpha$ represent a shift vector that helps to approximate $\\alpha$ with $\\alpha^s$. \n",
    "\n",
    "From here it should be evident that $\\alpha^s$ approximates $\\alpha$ better and better as the $\\nabla \\alpha$ gets smaller, so in practice when $\\nabla \\alpha$, gets smaller than a threshold, we stop iterating.\n",
    "\n",
    "The shift vector plays its part when we expand the $k(\\alpha)$ using the taylor series:\n",
    "$$k(\\alpha^s + \\nabla \\alpha) \\approx k(\\alpha^s) + k'(\\alpha^s)\\nabla \\alpha + \\frac{k''(\\alpha^s)(\\nabla \\alpha)^2}{2!}$$\n",
    "\n",
    "Here we are expanding until the second order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9cdcd8-54ff-4351-9e13-f1e45838dddc",
   "metadata": {},
   "source": [
    "Now, the idea is that the minimum of this approximation should be the minimum of the $k$, so we try to find the minimum value of this approximation. \n",
    "The method is similar to above. We set derivative of $k$ to 0.\n",
    "In this case, since the value $\\alpha^s$ is provided during the iteration, we compute the derivative against $\\nabla \\alpha$:\n",
    "\n",
    "$$\\frac{\\partial k(\\alpha^s + \\nabla \\alpha)}{\\nabla \\alpha} = 0 =\n",
    "0 + k'(\\alpha^s)+ \\frac{2 * k''(\\alpha^s) \\nabla \\alpha}{2}$$\n",
    "\n",
    "The equation simplifies as:\n",
    "$$0 = k'(\\alpha^s) + k''(\\alpha^s) \\nabla \\alpha$$\n",
    "and consequently:\n",
    "$$-k'(\\alpha^s) = k''(\\alpha^s) \\nabla \\alpha$$\n",
    "The $-k'(\\alpha^s)$ is the gradient of $k$ evaluated at $\\alpha^s$.\n",
    "The $k''(\\alpha^s)$ is the hessian matrix of $k$ evaluated at $\\alpha^s$.\n",
    "Hence the final form of the equation:\n",
    "$$-g(\\alpha^s) = H(\\alpha^s) \\nabla \\alpha$$\n",
    "\n",
    "This is classic $Ax=b$ type equation and can be solved using decomposition strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9eb2e8-27f5-43b9-b047-062ff9794e89",
   "metadata": {},
   "source": [
    "The $g(\\alpha^s) \\in \\mathbb{R}^n$ is a vector:\n",
    "$$g_k(\\alpha^s) = [\\frac{\\partial k}{\\alpha^s_1}, \\dots, \\frac{\\partial k}{\\alpha^s_n}]$$\n",
    "\n",
    "The $H(\\alpha^s) \\in \\mathbb{R}^{n \\times n}$ is a matrix based on the gradient:\n",
    "$$H_k(\\alpha^s) = \\begin{pmatrix}\n",
    "\\frac{\\partial^2 k}{\\partial (\\alpha^s_1)_1} & \\frac{\\partial^2 k}{\\partial (\\alpha^s_1)_2} & \\dots & \\frac{\\partial^2 k}{\\partial (\\alpha^s_1)_n} \\\\\n",
    "\\frac{\\partial^2 k}{\\partial (\\alpha^s_2)_1} & \\frac{\\partial^2 k}{\\partial (\\alpha^s_2)_2} & \\dots & \\frac{\\partial^2 k}{\\partial (\\alpha^s_2)_n} \\\\\n",
    "\\dots \\\\\n",
    "\\frac{\\partial^2 k}{\\partial (\\alpha^s_n)_1} & \\frac{\\partial^2 k}{\\partial (\\alpha^s_n)_2} & \\dots & \\frac{\\partial^2 k}{\\partial (\\alpha^s_n)_n}\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10294d60-373b-4bf0-8a14-78c67b51db04",
   "metadata": {},
   "source": [
    "The problem is computing the $H$ takes a lot of time, so in reality one would approximate it using the jacobian like: $$J(\\alpha^s)J^T(\\alpha^s) \\nabla \\alpha = -g(\\alpha^s) \\nabla \\alpha$$\n",
    "This is called the normal equation or *Gauss-Newton* equation.\n",
    "\n",
    "Its incomplete derivation is given below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f86c5-c350-48ef-bcdb-566e30fd74b6",
   "metadata": {},
   "source": [
    "## Deriving Gauss-Newton equation\n",
    "\n",
    "Consider $m$ data points: \n",
    "$(x_1, y_1), (x_2, y_2), (x_i, y_i), \\dots (x_m, y_m)$ where $x_i, y_i \\in \\mathbb{R}^n$\n",
    "\n",
    "The problem is stated as minimizing $S$ in $S = \\sum_{i=1}^m (r_i)^2$ where $r_i = y_i - f(x_i, \\beta)$.\n",
    "\n",
    "The minimum value occurs when gradient is 0.\n",
    "Here the $x_i$ and $y_i$ are given so the only term we can manipulate for finding the minimum is effectively $\\beta$.\n",
    "\n",
    "Notice that we have three functions $S$, the objective function, $(r_i)$ the residual function and $f$ the model function.\n",
    "We are interested in the minimum of $S$, so we set its derivative to 0:\n",
    "$S'(\\beta) = 0$. Let's rewrite the objective function as a composition of other functions to better express the derivative through chain rule:\n",
    "$$S(\\beta) = K(r(f(\\beta)))$$ where $K(r) = \\sum_{i=1}^m r^2$, $r(\\hat{y}_i) = y_i - \\hat{y}_i$ and $\\hat{y}_i = f(x_i, \\beta)$\n",
    "\n",
    "In function composition notation $S(\\beta) = (K \\circ r \\circ f)(\\beta)$\n",
    "\n",
    "The derivative of the composition function would be:\n",
    "$$S'(\\beta) = K'((r \\circ f)(\\beta)) * (r \\circ f)'(\\beta)$$\n",
    "\n",
    "The derivative of second composition $(r \\circ f)$ would be:\n",
    "$$(r \\circ f)'(\\beta) = r'(f(\\beta)) * f'(\\beta)$$\n",
    "\n",
    "The final form of the derivative would be:\n",
    "$$S'(\\beta) = K'((r \\circ f)(\\beta)) * r'(f(\\beta)) * f'(\\beta)$$\n",
    "\n",
    "or in more familiar terms\n",
    "$$S'(\\beta) = K'(r) * r'(\\hat{y}) * f'(\\beta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c012e521-8bca-4c1c-a04e-795077473436",
   "metadata": {},
   "source": [
    "Let's plug the terms to back the equation step by step:\n",
    "$$K'(r) = \\nabla (\\sum_{i=1}^m r^2) = 2 \\sum_{i=1}^m r$$\n",
    "\n",
    "$$r'(\\hat{y}) = \\nabla (y_i - \\hat{y}) = -1$$ where $\\hat{y} = f(\\beta)$\n",
    "\n",
    "$$f'(x, \\beta) = \\begin{pmatrix}\n",
    "  \\frac{\\partial x_1}{\\beta_1} & \\frac{\\partial x_1}{\\beta_2} & \\dots & \\frac{\\partial x_1}{\\beta_j} & \\dots & \\frac{\\partial x_1}{\\beta_n} \\\\\n",
    "  \\frac{\\partial x_2}{\\beta_1} & \\frac{\\partial x_2}{\\beta_2} & \\dots & \\frac{\\partial x_2}{\\beta_j} & \\dots & \\frac{\\partial x_2}{\\beta_n} \\\\\n",
    "  \\dots \\\\\n",
    "  \\frac{\\partial x_i}{\\beta_1} & \\frac{\\partial x_i}{\\beta_2} & \\dots & \\frac{\\partial x_i}{\\beta_j} & \\dots & \\frac{\\partial x_i}{\\beta_n} \\\\\n",
    "  \\dots \\\\\n",
    "  \\frac{\\partial x_m}{\\beta_1} & \\frac{\\partial x_m}{\\beta_2} & \\dots & \\frac{\\partial x_m}{\\beta_j} & \\dots & \\frac{\\partial x_m}{\\beta_n}\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d8da4a-5b6b-4456-aa94-ccc020c49041",
   "metadata": {},
   "source": [
    "The model function $f(x, \\beta)$ can be expanded using the first order taylor series: $$f(x_i, \\beta) \\approx f(x_i, \\beta^k) + f'(x_i, \\beta^k)*(\\beta - \\beta^k)$$ where $\\beta^k$ represents the state of a value provided to $\\beta$ during the beginning of approximation at the $k$th iteration.\n",
    "In an alternative formulation, let $\\beta = \\beta^k + \\nabla \\beta$:\n",
    "$$f(x_i, \\beta^k + \\nabla \\beta) \\approx f(x_i, \\beta^k) + f'(x_i, \\beta^k) * \\nabla \\beta$$\n",
    "\n",
    "The $f'(x, \\beta^k)$ is essentially \n",
    "$$ J(f) = f'(x, \\beta^k) = \\begin{pmatrix}\n",
    "  \\frac{\\partial x_1}{\\beta^k_1} & \\frac{\\partial x_1}{\\beta^k_2} & \\dots & \\frac{\\partial x_1}{\\beta^k_j} & \\dots & \\frac{\\partial x_1}{\\beta^k_n} \\\\\n",
    "  \\frac{\\partial x_2}{\\beta^k_1} & \\frac{\\partial x_2}{\\beta^k_2} & \\dots & \\frac{\\partial x_2}{\\beta^k_j} & \\dots & \\frac{\\partial x_2}{\\beta^k_n} \\\\\n",
    "  \\dots \\\\\n",
    "  \\frac{\\partial x_i}{\\beta^k_1} & \\frac{\\partial x_i}{\\beta^k_2} & \\dots & \\frac{\\partial x_i}{\\beta^k_j} & \\dots & \\frac{\\partial x_i}{\\beta^k_n} \\\\\n",
    "  \\dots \\\\\n",
    "  \\frac{\\partial x_m}{\\beta^k_1} & \\frac{\\partial x_m}{\\beta^k_2} & \\dots & \\frac{\\partial x_m}{\\beta^k_j} & \\dots & \\frac{\\partial x_m}{\\beta^k_n}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "It is evident that $\\beta^k$ approximates $\\beta$ better and better as the $\\nabla \\beta$ gets smaller, ( second term in plus gets smaller so the perturbation gets smaller ), so in practice when $\\nabla \\beta$, also known as the shift vector, gets smaller than a threshold, we stop iterating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ddd1ea-5d62-4a26-a62f-73f31f48ea54",
   "metadata": {},
   "source": [
    "Now let's rewrite the final form of the derivative after the taylor expansion:\n",
    "$$S'(\\beta) = 2 \\sum_{i=1}^m r * -1 * f'(x, \\beta^k)$$ where $*$ stands for matrix multiplication. \n",
    "\n",
    "We can also substitute the $f(x, \\beta^k)$ in $r$ to govern the $y$ residual through $\\beta^k$:\n",
    "$$S'(\\beta) = 2 \\sum_{i=1}^m (y_i - (f(x_i, \\beta^k) + f_i'(x, \\beta^k) * \\nabla \\beta) * -1 * f'(x, \\beta^k)$$ where $f'_i(x, \\beta^k)$ represents the ith row vector in the jacobian $J(f)$ matrix.\n",
    "\n",
    "When we distribute the minus sign, we get:\n",
    "$$S'(\\beta) = 2 \\sum_{i=1}^m (y_i - f(x_i, \\beta^k) - f_i'(x, \\beta^k) * \\nabla \\beta) * -1 * f'(x, \\beta^k)$$\n",
    "\n",
    "Since we are looking for $S'(\\beta) = 0$ due to minimization requirement, this equation morphs into:\n",
    "$$0 = 2 \\sum_{i=1}^m (y_i - f(x_i, \\beta^k) - f_i'(x, \\beta^k) * \\nabla \\beta) \\cdot -1 * f'(x, \\beta^k)$$\n",
    "\n",
    "Now let's move the negative sign to the coefficient:\n",
    "$$0 = -2 \\sum_{i=1}^m (y_i - f(x_i, \\beta^k) - f_i'(x, \\beta^k) * \\nabla \\beta) * f'(x, \\beta^k)$$\n",
    "\n",
    "Let $\\nabla y_i = y_i - f(x_i, \\beta^k)$, then the equation morphs into:\n",
    "$$0 = -2 \\sum_{i=1}^m (\\nabla y_i - f_i'(x, \\beta^k) * \\nabla \\beta) * f'(x, \\beta^k)$$\n",
    "\n",
    "Notice that right side of the equation is all multiplications, so we can get rid of the coefficient -2 at this point:\n",
    "$$0 = \\sum_{i=1}^m (\\nabla y_i - f_i'(x, \\beta^k) * \\nabla \\beta) * f'(x, \\beta^k)$$\n",
    "\n",
    "We distribute the $f'(x, \\beta^k)$ to summation terms:\n",
    "$$0 = \\sum_{i=1}^m \\nabla y_i * f'(x, \\beta^k) - \\sum_{i=1}^m f_i'(x, \\beta^k) * \\nabla \\beta * f'(x, \\beta^k))$$\n",
    "\n",
    "We move the second summation to the left side of the equation and achieve the final form of the equation ?:\n",
    "$$\\sum_{i=1}^m f_i'(x, \\beta^k) * \\nabla \\beta * f'(x, \\beta^k) = \\sum_{i=1}^m \\nabla y_i * f'(x, \\beta^k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20d78ba-5df6-4ed2-82fe-0fc2a2cb4a4a",
   "metadata": {},
   "source": [
    "\n",
    "The term $f_i'(x, \\beta^k) * \\nabla \\beta$ is effectively:\n",
    "$f_i'(x, \\beta^k) * \\nabla \\beta = \\frac{\\partial x_i}{\\beta^k_1} * \\nabla \\beta_1 + \\dots \\frac{\\partial x_i}{\\beta^k_n} * \\nabla \\beta_n$\n",
    "\n",
    "From here one can see that $\\nabla \\beta$ distributes to summation like:\n",
    "$$(\\frac{\\partial x_1}{\\beta^k_1} + \\dots + \\frac{\\partial x_m}{\\beta^k_1}) \\nabla \\beta^k_1 + \\dots + (\\frac{\\partial x_1}{\\beta^k_n} + \\dots + \\frac{\\partial x_m}{\\beta^k_n}) \\nabla \\beta^k_n$$\n",
    "\n",
    "This distribution can be abbreviated as the following:\n",
    "$$\\sum_{j=1}^n \\sum_{i=1}^{m} J_{ij}\\nabla \\beta_j$$\n",
    "\n",
    "The equation becomes:\n",
    "$$\\sum_{j=1}^n \\sum_{i=1}^{m} J_{ij}\\nabla \\beta_j * J(f) = \\sum_{i=1}^m \\nabla y_i * J(f)$$\n",
    "\n",
    "We multiply both sides with the inverse of jacobian:\n",
    "$$\\sum_{j=1}^n \\sum_{i=1}^{m} J_{ij}\\nabla \\beta_j * J(f) * J^{-1}(f) = \\sum_{i=1}^m \\nabla y_i * J(f) * J^{-1}(f)$$\n",
    "\n",
    "Since matrix multiplication has associative property when dimensions match, we can do $J(f) * J^{-1} = I$ and substitute it to the equation:\n",
    "$$\\sum_{j=1}^n \\sum_{i=1}^{m} J_{ij}\\nabla \\beta_j * I = \\sum_{i=1}^m \\nabla y_i I$$ where $I$ represents the identity matrix. Then we have the final form of the equation:\n",
    "$$\\sum_{j=1}^n \\sum_{i=1}^{m} J_{ij}\\nabla \\beta_j = \\sum_{i=1}^m \\nabla y_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f565e29-4db5-4144-9559-462ae1bd6429",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea98d3f-5ce0-4866-a900-79d86bad137c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
