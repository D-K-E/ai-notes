{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546f86c5-c350-48ef-bcdb-566e30fd74b6",
   "metadata": {},
   "source": [
    "# Non linear Optimization\n",
    "\n",
    "State estimation in SLAM depends on the following two equations:\n",
    "\n",
    "- $x_k = f(x_{k-1}, u_k) + w_k$\n",
    "- $z_{k,j} = h(y_j, x_k) + v_{k,j}$\n",
    "\n",
    "The $x_k$ is the camera pose in 6d, $z_{k,j}$ is the image position of the observation $y_j$.\n",
    "The $u_k$ is the input data at time $k$. \n",
    "The input data can be a set of 2d points that are known to belong to the same object in a sequence of images.\n",
    "It can be a mixed set of 3D and 2D points, or a set of 3D points (see Camera-Motion notebook for the related discussion).  \n",
    "\n",
    "The $x_k$ can be expressed as $R_k y_j + t_k$. \n",
    "Under this expression the $x_k$ and $z_{k,j}$ is bound with the following equation:\n",
    "$$s z_{k,j} = K(R_k y_j + t_k) = KT y_j$$\n",
    "where $T \\in SE3D$, $s$ represents the distance of pixels, and $K$ represents the intrinsic camera matrix.\n",
    "\n",
    "The $w_k$ and $v_{k,j}$ are noise terms. They are usually assume to be gaussian with 0 mean.\n",
    "\n",
    "Given the presence of noise, our problem can be formulated as a conditional probability distribution like $P(x, y | z, u)$ meaning that given the pixel position $z$ and input $u$ what is the probability of pose being $x$ and observation being $y$. \n",
    "\n",
    "The values that maximize this probability distribution minimize the error and noise in the system.\n",
    "From the bayes rule, $argmax P(x, y | z, u) = argmax P(z, u | x, y) * P(x, y)$, since we may not know the prior $P(x,y)$ we can also ignore that and transform the Maximizing posterior distribution problem to Maximium likelihood estimation problem, and end up with $$argmax P(x, y| z, u) = argmax P(z, u | x, y)$$.\n",
    "\n",
    "This means that we need to minimize the following terms:\n",
    "- $e_{u, k} = x_k - f(x_{k-1}, u_k)$\n",
    "- $e_{z, j, k} = z_{k, j} - h(x_k, y_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c7b873-0352-4535-8ecf-fda8ecab6005",
   "metadata": {},
   "source": [
    "How do we minimize these terms ? Well, assuming that $e_{u,k}$ defines a loss function $f_{loss}$, we are looking for a way to find the smallest output from the function $e_{u, k} = f_{loss}(x_k, x_{k-1}, u_k)$.\n",
    "\n",
    "Formally, we want to order our loss function using the following partial ordering: $f_{loss}(a + \\nabla a) < f_{loss}(a)$. \n",
    "\n",
    "This is actually a fairly well known problem, if one wants to obtain this ordering, one needs to advance at the opposite direction of the gradient $\\nabla a = -J(a)$ where $J$ is the jacobian matrix containing the first order derivative of the $f_{loss}(a)$ function. \n",
    "The problem here is that we need to do iterative updates until we find the $\\nabla a$.\n",
    "The iterative updates requires a step size, in machine learning it is called the learning rate, this sometimes get stuck in a local minima and fails to give the global minimum.\n",
    "Fortunately there is a solution to this.\n",
    "\n",
    "We are looking for $\\nabla a$ in the $f_{loss}(a + \\nabla a)$, one of the ways to find it is to expand the $f_{loss}(a)$ using Taylor series:\n",
    "$$f_{loss}(a) = f_{loss}(a) + \\nabla a f'_{loss}(a) + \\frac{(\\nabla a)^2 f_{loss}(a)}{2!} + \\dots$$\n",
    "\n",
    "More succinctly $$f_{loss}(a) \\simeq f_{loss}(a) + \\nabla a J(a) + \\frac{(\\nabla a)^2 H(a)}{2}$$ \n",
    "where $J$ represents the jacobian (first order derivative) matrix, and $H$ represents the hessian (second order derivative) matrix.\n",
    "Now we want to minimize the right side of this equation. \n",
    "Let's take the derivative of the $\\nabla a$:\n",
    "$$k(\\nabla a) = f_{loss}(a) + \\nabla a J(a) + \\frac{(\\nabla a)^2 H(a)}{2}$$\n",
    "$$k'(\\nabla a) = 0 + J(a) + \\frac{2 * \\nabla a H(a)}{2}$$\n",
    "$$k'(\\nabla a) = J(a) + \\nabla a H(a)$$\n",
    "\n",
    "The minimum/maximum value of this function can be found with setting the derivative to 0:\n",
    "$$argmin(k(\\nabla a)) = ( k'(\\nabla a) = 0) = J(a) + \\nabla a H(a)$$\n",
    "$$J(a) + \\nabla a H(a) = 0$$\n",
    "hence\n",
    "$$\\nabla a H(a) = -J(a)$$\n",
    "\n",
    "Notice that this has the classic $Ax=B$ form and can be solved for $x$ using LU decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10294d60-373b-4bf0-8a14-78c67b51db04",
   "metadata": {},
   "source": [
    "The problem is computing the $H$ takes a lot of time, so in reality one would approximate it using the jacobian like: $$J(a)J^T(a) \\nabla a = -J(a)f_{loss}(a)$$\n",
    "This is called the normal equation or *Gauss-Newton* equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2400cb-1217-4c63-80d6-cd289ee0d7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
