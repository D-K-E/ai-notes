##########################
Recurrent Neural Networks
##########################

When to use RNNs ?

RNNs are good with time series.
Predicting stock market prices,
speech recognition, etc.

CNNs are good for images and videos, RNNs are good for sequential data analysis
like financial time series.

RNNs are used for speech recognition and machine translation.

RNNs deal with ordered sequences

It can be used to autocomplete stuff. Like you have a phrase:

"My cat is the" and then the trained algorithm can fill the phrase
as "My cat is the **best**"

In this case,
the input is the ordered sequence of words or characters
the output is ordered sequence of characters.

Machine translation:
the input is ordered sequence of words in language X
the output is ordered sequence of words in language Y

Vanilla supervised learners and structured input
-------------------------------------------------

Vanilla supervised learnes, like feedforward networks take generic input and
spit out a structured input.
They are basically function estimators.
Their key problem is that they do not consider the input structure at all.

Now CNNs were a change to that since they detected the patterns, that is they
take into consideration the spatial corelation between the pixels in an image.

RNNs are designed to exploit ordered sequential structures. Basically time
series are a good subject for these.

RNN Notations
--------------

What is ordered sequence ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ordered sequence is a list of values ordered by an index.
Index can be a list of numbers, or timestamps, or anything really.

How to model ordered sequences ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Most ordered sequences are a product of some underlying process or processes,
but these underlying processes can be blackboxes, so we have no model who
explains the data.

In this case we can model the process recursively, that is we can use past
values to predict the future ones, meaning we can try to determine how
future values mathematically dependent on the preceeding ones.

What are seeds ?
~~~~~~~~~~~~~~~~~

Let's say we are dealing with the set of odd numbers:

:math:`[1,3,5,7, ..., ]`

First value is :math:`S_1=1`.
That is it is a given.
However all the other values can be generated from the first one by adding 2.
This means that odd numbers set is a recursive sequence.
That is the rest of the sequence can be generated by a model from one or more
original values. The original values need to be given.

These initialized/given values in the case of a recursive sequence, are called
seeds

What is the order of a recursive sequence ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The order of a recursive sequence is the number of prior elements it uses to
produce future ones

For example, the odd number set has the order of 1. Fibonacci sequence has the
order of 2.

Unfolded view
~~~~~~~~~~~~~~

The odd numbers set can actually be described by using the following function.

:math:`f(s) = s + 2`

or as follows:

s_1 --f--> s_2 --f--> s_3 --f--> s_4 --f--> ...

The name unfolded view applies both of these representations, where we show
explicitely how each element of the sequence is generated.

Folded view
~~~~~~~~~~~~

The odd numbers set can actually be described by using the following notation.

:math:`s_t = f(s_{t-1})` 

Important note: A recursive sequence simply applies the same function to its
output to generate the new input

How to drive recursive sequence
---------------------------------

Let's try to model how much money we have at the end of each month:

I need to know:

- The current value of my savings = h_1
- Savings of each month would be = h_t for the month t

What drives my account ?

- Rent we pay, car lease, food shopping etc, drive it down.
- Paycheck for the job, small investments etc, drive it up.

- s_t = is my loss or income during the month, so a sum of downers and uppers

Simple model for monthly savings would be:

- h_1 = 0
- h_2 = h_1 + s_1

The folded view of the model would be:

- h_t = h_{t-1} + s_{t-1}
- where t = 2,3, ...

Driver sequence is what moves everything, here it corresponds to monthly income
or loss.
The hidden sequence is that which is generated by the driver sequence, here
corresponds to h_t

Injecting recursivity to supervised learners
----------------------------------------------

Suppose we have a set [1,3,5,7,9,11,13,15], we only have this set and nothing
else. How do we arrive at the function f(s_t) = s_{t-1}+2
We can't use sheer guessing. We need to find a function that generates the
sequence.

We can use a parametrized function and learn weights by fitting:

s_1 = 1
s_2 = g(s_1), where g(s_t) = w_0 + w_1{\times}s_{t-1} 
s_3 = g(s_2)

Now we don't know the weights for w_0 and w_1. We simply implement least squares
cost function to find that.

For example:

least square for s_2 is (s_2 - g(s_1))^2
least square for s_3 is (s_3 - g(s_2))^2, etc.

We then try to minimize the sum of these least squares.

Formally :math:`min({\sum_{t}^{N}(s_t - (w_0 + w_1{\times}s_{t-1}))^2})`

We are simply performing regression here.

Windowing a sequence
--------------------

During the above formula we have looked at pairs. For example
(s_1,s_2), (s_2, s_3), etc. The reason we chose pairs was related to
the formula we adopted, g(s_t) = w_0 + w_1{\times}s_{t-1}. These pairs
are called windows, that is the parameters involved in a given iteration
of the formula of the recursive sequence, establishes the window of that
iteration.

Keras and fitting
------------------

The above given model can be defined as follows in Keras:

.. code-block:: python3

   model = Sequential()
   layer = Dense(1, input_dim=1, activation="linear")
   model.add(layer)
   model.compile(loss="mean_squared_error", optimizer="adam")
   model.fit(x,y, epocha=3000, batch_size=3,
                callbacks=callbacks_list, verbose=0)

The point here is the following:

Given an ordered sequence, we can make a recursive approximation to it by first
making a random guess about the architecture of its recursive formula, then
tuning the parameters of that architecture optimally using the sequence itself.


We could have used the model in the traditional manner: Use a training set,
validation set and test set, etc. So we can make predictions based on the
training set.

Thus we can actually use these networks as generative models.

General Notes Recursivity in Supervised Learning
------------------------------------------------

There might be several architectures that model the recursive sequence which
generated from a single recursive function.

The sequence itself might not be recursive but we might find a recursive
function that fits the sequence at hand, that is we can find the best
recursive approximation to the given sequence.


Why RNN instead Feedforward neural networks
--------------------------------------------

The cost function we are using in the feed forward neural networks is Least
Squares and from the perspective of probablilities this means that, we assume
that the probability distribution is identically distributed among the
independent elements of the set.

However it is a little contradictory to the fact that we are assuming that these
elements are part of an ordered sequence even a recursive one, and that they
don't emit any structure in their distribution in the set.

Basically if we were dealing with odd numbers set, we are assuming that,
3 and 5 are conditionally independent to each other, which is misleading.
Because if I change 3 the value of all the others in the sequence would also
change.

Feed forward networks structure their data as data points with no edges between
however, we *know* that there are, that's the whole point of referring to them
as recursive sequences.

Simply put:

Feed forward neural networks use recursive approximation:
- Try to model dependency -> recursiveness
- But when try to tune parameters -> we make things independent since we assume
  that they are actually independent.

Basic RNN derivation
=====================

With Feed forward neural networks we modelled the recursivity but lost the
dependence while tuning the parameters.

RNNs are there to enforce dependency between the levels. The way to do this
is to ensure that each level ingest its predecessor functionaly.
It does so by taking 2 arguments, both the sequence element and the hidden
state.

Here is an unfolded view of a simple RNN architecture

.. math::

   s_1 {\approx} {\hat{s_1}} = {\alpha}
   s_2 {\approx} {\hat{s_2}} = g({\hat{s_1}}, s_1)
   s_3 {\approx} {\hat{s_3}} = g({\hat{s_2}}, s_2)
   s_4 {\approx} {\hat{s_4}} = g({\hat{s_3}}, s_3)
   {\vdots}
   s_t {\approx} {\hat{s_t}} = g({\hat{s_{t-1}}}, s_{t-1})

Notation:

- :math:`s_t where t=1,2,3,4, {\dots}` are the elements of our recursive
  sequence
- :math:`{\hat{s_t}} where t=1,2,3,4, {\dots}` are the hidden states
  that are driven by the sequence elements, basically they are the ones
  that are calculated from the sequence elements.
- :math:`{\alpha}` is the seed value

Dependency is much more explicit here, since each hidden state depends
functionally on the preceeding state.

Modifying least square loss
----------------------------

The idea stays the same, we simply plug in our functionally dependent levels
to least squares loss, like the following


For example:

least square for s_2 is (s_2 - g({\hat{s_1}}, s_1))^2
least square for s_3 is (s_3 - g({\hat{s_2}}, s_2))^2, etc.

In most cases we also use the linear combination of the hidden state with the
sequence element as the second term in the subtraction. This is heavily used
in the case of RNNs but there are no formal justifications for it.
This modifies the loss function in the following way:

.. math::

   min(
   {\sum_{t}^{N}(s_t - (w_0 + w_1{\times}g({\hat{s_{t-1}}}, s_{t-1}) ))^2}
   )

Keras and RNN
---------------

The above given model can be defined as follows in Keras:

.. code-block:: python3

   model = Sequential()
   layer = SimpleRNN(3, input_shape=(2,1), activation="relu")
   model.add(layer)
   model.add(Dense(1))
   model.compile(loss="mean_squared_error", optimizer=opt)

RNN and memory
----------------

Every level of rnn contains the complete history of every sequence value
that precedes it. This why RNNs have memory.

Technical notes
----------------

- RNNs need large  datasets to function well.
- Vanishing gradient problem stands for RNNs as well.
  - Different level architectures, for example Long Term Short Term memory
  - Variations on stochastic gradient descent
  - Longer sequence --> deeper network, and window as well

Implementing Character wise RNN
-------------------------------

Character wise RNN means the network will learn one character at a time and
generate new characters one character at a time.

Basically the network would take a character as an input and output a
probability distribution for the next likely character.

Sequence Batching
-----------------

We are working on sequences of data, by taking in a sequence an splitting it
into a multiple shorter sequences, we can take advantage of matrix operations
to make training more efficient.

batch size is the number of shorter sequences we are using
batch length is the length of sequences we are feeding into the network
