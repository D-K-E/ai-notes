##########
Manifolds
##########

Notes on manifolds taken from different sources. 
The first use case that pushed me to read upon manifolds is rendering of
normal maps.


Prerequisites
==============

Taken from Lee, John, Introduction to Topological Manifold, 2000, Springer

This is mostly a remainder on the fundamentals of the knowledge required to
understand the manifold theory.

Set Theory
-----------

Set is mathematically an undefined term. It should be thought of an assemblage
of mathematical objects. The relation between a set and its elements is called
membership. It is also mathematically undefined. However there is a notation
for it :math:`x \in S`, *x* being the element and *S* is the set. The sign
between them is read as *is a member of*, so the whole statement is read as *x
is a member of S*. 

A set is qualified by its members, so if *S=T* then :math:`x \in T` as well.
If a set *G* contains another set *K*, then this is denoted by 
:math:`K \subset G`.
If :math:`K \neq G`, then *K* is a *proper subset* of *G*.
If :math:`K = G` then :math:`K \subset G` and :math:`G \subset K`

- Specification Axiom: Given a set *S* and a proposition *P(x)* that is either
  true or false whenever x is any particular element of *S*, there is a set
  consisting of all those :math:`x \in S` for which *P(x)* is true. This is
  denoted by :math:`{x \in S: P(x)}`

- Power set Axiom: Given any set *S*, there is a set *P(S)*, called the power
  set of *S* whose elements are subset of *S*.

- Union Axiom:




Notes taken mostly from wikipedia

Topological Space
------------------

Now a topological space consist of a set of points and a set of neighbourhoods
for points. It also satisfies a set of axioms with respect to points and their
neighbourhoods. As you can see it is pretty general.

The axioms that define a topological space are the following:
Let X be a collection of n dimensional points.
Let N be a function whose domain is X and co domain is a subset of X.

- Each point belongs to every one of its neighbourhoods. 
  Formally, if :math:`A \in N(x)` then :math:`x \in A`

- Every superset of a neighbourhood of x is also a neighbourhood of x.
  Formally, let :math:`A \in N(x)`, if :math:`A \in B` and :math:`B \subset X` then
  :math:`B \in N(x)`

- The intersection of two neighbourhoods of x is also a neighbourhood of x.
  Formally let :math:`A, B \in N(x)` then :math:`(A âˆ© B) \in N(x)`

- Any neighbourhood A of x contains a neighbourhood B of x, such that A is
  also a neighbourhood of each point of B. Formally, :math:`A \in N(x)` and
  :math:`B \in N(x)`, and :math:`B \subset A`. If :math:`b \in B` then :math:`A \in N(b)`

It can also be defined using open sets, which I find it to be a little easier
to visualize:
Let *X* be a collection of points. 
Let *U* be a subset of *X*. *U* is defined as open set if *U* is a
neighbourhood of all points in *U*. Then a topological space is an
ordered pair of *(X, t)* where *t* is a collection of subsets of *X*. *t*
satisfies the following axioms:

- The empty set and X itself belong to t.
- Any arbitrary members union of t belongs to t.
- Any intersection of members of t belongs to t.

For example: :math:`X = {1,2,3}` and a corresponding collection would be
:math:`U_1 = {{}, {1}, {2}, {1, 2}, {1, 2, 3}}` or it can be something like
:math:`U_2 = {{}, {2}, {1, 2}, {2, 3}, {1, 2, 3}}` as you can see the
collection *U* clearly contains *X* but it has a different structure than *X*.
In a manifold the members of such a collection had an equivalence in euclidean
space.

Manifold
---------

A manifold is simply a specific topological space where the neighbourhood of
each point is homeomorphic to euclidean space. 

Now the term homeomorphic means that there exists a continuous function
with an inverse continuous function between two topological spaces.
Formally there exists a function :math:`f: X \to Y` who satisfies the
following conditions:

- f is a bijection, meaning that every element of the set X has exactly one
  correspondent in set Y such that each element from both sets can not be
  paired with more than one element. This implies that both sets have the same
  number of elements and that each pair is unique, and the intersection of any
  pair is empty set.

- f is continuous, that is for each element of X there exists a mapping with
  value in Y 

- :math:`f^{-1}` is also continuous, that is for each value that is mapped by
  the function f, there exists a function that maps the value back to the
  domain of f. Formally if :math:`f(x) = y` for all :math:`x \in X` then for all
  :math:`y \in Y` :math:`f^{-1}(y) = x`.

Now with the given definition of homeomorphic we can understand easily the
definition of a manifold. Since a manifold is a topological space, it consists
of a set of points *X*, and a collection of subsets of *X*. These subsets are
homeomorphic to euclidean space, that means that they can be
mapped one-to-one and onto an euclidean space thus satisfying the above
mentioned conditions of homeomorphism. 

Now a subset in the collection, in the context of a manifold, is called a
*chart*. The entire collection is called an *atlas*. When we have a point that
appear in different neighbourhoods, we can have a function that maps from
neighbourhood to manifold and back to neighbourhood, these functions are
called *transition maps*.

Now let's see the whole thing in action:

Let's think of a unit circle, whose center is (0,0), with :math:`r=1`.
Let's divide it into 4 overlapping parts: top arc which covers for top half of
the circle, bottom arc, right arc, left arc, each covering the half of the
circle.

The each point on the top arc can be described its x coordinate. We can then
imagine a function, more specifically a projection, that maps a given
coordinate to an open interval (leftMostXCoordinate, rightMostXCoordinate),
meaning in our case (-1, 1). 
Why (-1,1) ? Because we are dealing with unit circle, that is centered on
(0,0).
Formally our function would be something like this: :math:`X_{top}(x, y)=x`.
For other arcs we would have the following:

- :math:`X_{top}(x,y) = x`
- :math:`X_{bottom}(x,y) = x`
- :math:`X_{left}(x,y) = y`
- :math:`X_{right}(x,y) = y`

These functions are called *charts* in the context of a manifold. They simply
represent a certain region of the manifold. They are also specific to that
region. Why ? Because their behaviour is local. :math:`X_{top}` behaves the
way it does because its domain is top arc, if its domain was, for example, left
arc, maybe the equivalency specified in its definition above would not hold
true.

Now the top half and right half have an overlapping part, a quarter at the
positive side of both axes.
Both :math:`X_{top}` and :math:`X_{bottom}` maps this overlapping part to an
(0,1) interval.

Now the question is how do we represent this common part with these two
functions.
We can represent it with a function which maps from the co-domain of `X_{top}`
to the co domain of `X_{right}`.
We would have for example :math:`T: (0,1) \to (0,1)`.
Why can we represent this common part with such a function ?
Let's examine what this function does:
Let :math:`k \in (0, ..., 1)`, :math:`T(k) = X_{right}(X^{-1}_{top}(k))`
So function *T* is composed of the inverse of :math:`X_{top}` and
:math:`X_{right}`.
The inverse of :math:`X_{top}`, maps the value back to the point on circle, and
:math:`X_{right}` maps that point coordinate to the given range again.

Let's see the function in action. Let's take a point from the common part in
question, for example :math:`P = (\cos{a}, \sin{a})`

- let :math:`a = 45`.
- :math:`\cos{a} = ~0.525`
- :math:`\sin{a} = ~0.851`
- :math:`P = (0.525, 0.851)`

- :math:`X_{top}(0.525, 0.851) = 0.525`
- :math:`X_{right}(0.525, 0.851) = 0.851`
- :math:`T(0.525) = X_{right}(X^{-1}_{top}(0.525))`
- :math:`T(0.525) = X_{right}(0.525, 0.851)`
- :math:`T(0.525) = 0.851`

As you can see the function captures the point in common part.
This function, in the context of manifolds, is called *transition map*. It
makes the transition between two charts.

Now in order to make a transition to differentiable manifold, we need to cover
the basics of differentiable functions. Please note that *charts* are simply a
collection of functions, and since we describe a manifold with a collection of
charts, differentiability of manifolds relate to the differentiability of the
charts that made up the collection which describes the manifold.

Differentiable Manifold
------------------------

Let's clarify what we mean by differentiable when we are talking about
manifolds.

A function :math:`f: P \subset R \to \mathbb{R}` is said to be differentiable at
:math:`p \in P` when we can compute its derivative, meaning that we have a
solution for the following operation:

- :math:`f^{(1)}(p) = \lim_{h \to 0} \frac{f(p+h) - f(p)}{h}`

This also implies that at the given point *p* the function is continuous.
However continuity does not imply differentiability.

If a function's first order derivative is continuous, that is for each value
in its domain we can compute its derivative, then the function is of class 1.
The notation for this is :math:`C^{1}`. If it is denoted to be of class 3,
meaning :math:`C^{3}` than its first order, second order and third order
derivatives are continuous. Generally speaking if a function is of class k
:math:`C^{k}`, it means that derivatives :math:`f^{(1)}, f^{(2)}, ..., f^{(k)}`
are all continuous. A smooth function is continuous for all positive integer
values of k and it is denoted by the class infinite, :math:`C^{\infty}`.

What if our function has more than one variables, like *f(x,y)*, how do we
compute the derivative then ?
The answer is using partial derivatives. The logic is quite the same as the
computation of derivatives. Let's see an example:

- :math:`f(x,y) = x^2 + yx + y^2` is our function.

- We can also think of this function as :math:`f(g_{x}(y))`, that is as a
  family.

- We define the :math:`g(k) = x^2 + xk + k^2`. We simply treat the x variable
  as a constant in the function *g*.

- The derivative of the function *g* is easier to compute now:

  - Since :math:`x^2` is just a constant now, it amounts to zero
    :math:`g^{(1)}(k) = xk + k^2`
  - :math:`g^{(1)}(k) = x + 2k`

- The derivative of our function is :math:`f^{(1)}(x,y) = x + 2y`

This is generalized as the following,

.. math::

    \frac{\partial{f}}{\partial{x_i}}(a_1, ..., a_n) = \lim_{h \to 0} 
    \frac{f(a_1, ..., a_i + h, ..., a_n) - f(a_1, ..., a_i, ..., a_n)}{h}

It reads as follows: For an n dimensional point :math:`a = (a_1, ..., a_n)`,
the partial derivative in the :math:`x_i` direction/dimension, is computed by
dividing the difference of the first term and point *a* to h who
approaches to 0. The first term is obtained by adding *h* to point :math:`a' = (a_1,
..., a_i + h, ..., a_n` at the given dimension.

Now we have the necessary ground work for understanding what makes a manifold
differentiable. It is basically the differential structure that makes a set M
a differentiable manifold.

The differential structure of a manifold means that our charts are
:math:`C^{k}` functions, that is their derivatives are continuous up to order
k, making them :math:`C^{k}` compatible.
The definition of compatibility is the following:

- Let M be the set of points for the manifold.
- Let m be an n dimensional point of M: 
  :math:`m \in M` and :math:`m = (x_1, ..., x_n)`

- Let :math:`f_i = W_i \subset M \to U_i \subset \mathbb{R}^{n}`
- Let :math:`f_j = W_j \subset M \to U_j \subset \mathbb{R}^{n}`
- Intersection of the domains of these two would be 
  :math:`W_{ij} = W_i \cap W_j`
- :math:`U_{ij} = f_{i}(W_{ij})` and :math:`U_{ji} = f_{j}(W_{ij})`

- :math:`t_{ij}: U_{ij} \to U_{ji}`
- :math:`t_{ij} = f_{i}(f_{j}^{-1}(m)`

Here :math:`t_{ij}` is the transition map. The compatibility requirement is
that :math:`U_{ij}` and :math:`U_{ji}` are open, that is they are not bounded,
and that transition maps :math:`t_{ij}` and :math:`t_{ji}` have continuous
partial derivatives of order *k*. If this holds true for all transition maps
and charts etc, than the manifold is a differentiable manifold.

==================================
Geometry of Exponential Families 
==================================

Murray, M. K., and John W. Rice. 1993. Differential Geometry and Statistics. 1st ed. Monographs on Statistics and Applied Probability 48. Londonâ€¯; New York: Chapman & Hall.


Parametric statistics concerns itself with parametrised probability distributions
:math:`p(\theta)` where parameter \theta = (\theta^1, \theta^2, ..., \theta^d) which varies over R^d

For example gaussian family: :math:`N = p(m, \sigma^2) = 1/\sqrt(2\pi \sigma) e^{\frac{-(x-m)^2}{2 \sigma^2} }`
\theta = (m,\sigma) domain is R^2.

As a set of probability measures gaussian family can be characterised as:
:math:`N = {p(m, \sigma)dx | m \in R, \sigma \in R^{+}, dx \in R}`

Statistical inference involves data generated by sampling a space according to a
probability distribution which is a member of family of distributions. The
family of distributions can be briefly written as :math:`p(\theta)`.

Dependence on parameters during inference comes from the use of differential toolkit.

Let's study geometric characteristics of probability distributions. The most
famous parametric distribution for its geometric properties is the exponential
family:
:math:`p(\theta) = exp(\theta^1x^1 + ... + \theta^r x^r - K(\theta))dn`

- :math:`x^1` etc are random variables

- :math:`\theta^1` etc parameters

- :math:`n` is a measure on some sample space

- :math:`K` is an arbitrary function, helps us to normalize the distribution.

Let :math:`f` be a function whose domain is :math:`\{\theta^1x^1, ..., \theta^Nx^N\}`
if f or K is known, one can find the other one since it the other
one must a assume a form that causes the probability distribution to be
normalized.

When considering families of probability distributions geometrically,
distribution families are surfaces, probability distributions are points,
their parameters are coordinates:


+=============================+
|                             |
|   +-------------------+     |
|   |    family of      |     |
|   |   probability     |     |
|   |   distribution    |     |
|   |                   |     |
|   |   p_1             |     |
|   |  *(t_1, t_2,...)  |     |
|   |                   |     |
|   |                   |     |
|   |   p_2             |     |
|   |  *(k_1, k_2,...)  |     |
|   |                   |     |
|   +-------------------+     |
|                             |
+=============================+


Example Gaussian family
+=============================+
|                             |
|   +-------------------+     |
|   |    gaussian       |     |
|   |   probability     |     |
|   |   distribution    |     |
|   |   family          |     |
|   |                   |     |
|   |   p_1             |     |
|   |  *(mu_1, sigma_1) |     |
|   |                   |     |
|   |                   |     |
|   |   p_2             |     |
|   |  *(mu_2, sigma_2) |     |
|   |                   |     |
|   +-------------------+     |
|                             |
+=============================+

Unless we are considering the relationship between individual points, ie
probabilities, we don't need to consider the geometric shape that occurs in
the plane. However IF we consider the relationship between individual points,
for example between $p_1(\mu, \sigma)$ and $p_2(\mu, \sigma)$, we don't need
to consider the geometric shape of the resulting surface, ie. distribution
family.


Some notions concerning shapes come from using different geometry or
coordinate systems. For example, when we are talking about the distance
between two points of triangle, the notion distance is defined under the
assumption that the space in which we are dealing with the triangle has
coordinates. Thus the notion of coordinates which is imposed by the usage of
euclidean/affine geometry impacts our analysis with respect to the shape at
hand.

Here is the thing though, the triangle is also a triangle BECAUSE the
notion of distance affects it. For example on a surface defined on cartesian
coordinate system, what is the difference between a Point, and a Triangle, the
Point can not be associated with the notion of distance in the way a Triangle
can. Thus the notion of distance is constitutive difference between a Triangle
and a Point.

Now when we are dealing with a certain probability distribution that is not
expressed in exponential form, it does not necessarily mean that the
distribution is not exponential. It has to be PROVED that the distribution can
not be parametrised/expressed in exponential form.

### Canonical Coordinates

Let's write an exponential distribution:

$$ p(x, \theta) = \exp( \sum_{i=1}^r x^i \theta^i - K(\theta)$$

canonical parameters of this distribution is 
$\theta = {\theta^1, \theta^2, ..., \theta^r} \in R^r$

Here is the question, how canonical is $\theta$ ? 
First how do we know that the parameter is a canonical parameter. We know that
a certain parameter group is canonical by its invariant property which is
expressed in terms of restrictions over the set of values. Basically we ask
the following: Can we switch back to these coordinates after a set of
transformations. For example, 

- what if I have another function J(\theta):
  - $ p(x, \theta) = \exp( \sum_{i=1}^r x^i \theta^i - J(\theta)$

- what if I have another random variable y^i:
  - $ p(x, \theta) = \exp( \sum_{i=1}^r y^i \theta^i - K(\theta)$

- or what if I have transformation over the parameter:
  - $ p(x, \theta) = \exp( \sum_{i=1}^r x^i \phi(\theta^i) - K(\theta)$

- or what if we have all of them:
  - $ p(x, \theta) = \exp( \sum_{i=1}^r y^i \phi(\theta^i) - J(\theta)$

Can we say the following:
 $$
 p(x, \theta)
 =  \exp( \sum_{i=1}^r x^i \theta^i - K(\theta) )
 =  \exp( \sum_{i=1}^r y^i \phi(\theta^i) - J(\theta) )
 $$

---

$$ p(x, \theta) = \exp( ( \sum_{i=1}^r y^i \phi^i(\theta) ) - J(\theta) + f(x) ) $$

$$ p(x, w) = b(w)\exp(<w, t(x)> - k(w)) $$

k: function whose domain is w and codomain is real number set.
t: function whose domain is x and codomain is $R^d$, the output is a vector.
b: function whose domain is x and codomain is real number set.

Here are the terms of this equation:

- $\exp$ exponential operator: $\exp(x) = e^x$

- $\sum$ summation function: $\sum_{i=1}^{n=3} i = 1 + 2 + 3$

- $x$: a random variable: Random variable is a real valued function defined over an
  interval with a probability distribution. Random variable is a measurable
  function $X: \Omega \to E$. $\Omega$ sample space, $E$
  measurable space. $\Omega = (\omega, F, P)$ $\omega$ set of possible
  outcomes, $F$ event space, $P$ probability function with a real value
  between [0, 1].

- Y: a set of random variables whose members are defined over x, that is 
  $Y = g(x)$ where $g: R \to R$. The cumulative distribution function of Y must
  satisfy the following: $F_{Y}(y) = P(g(X) \le y)$

- $y^i$: member i of the set of random variables Y.

- $\theta$: parameter set of a probability distribution

- $\theta^i$: member i of the parameter set $\theta$

- $\phi$: is a set of functions that are defined over the parameter set theta.

- $\phi^i$ is a function whose domain is parameter set $\theta$, whose
  codomain is real number set.

- $J(\theta)$ is a function whose domain is parameter set $\theta$ and whose
  codomain is real number set.

- $f$ is a function whose domain is the output of random variable x, and whose
  codomain is real number set.

So the function $p(x, \theta)$ outputs a real number.

Canonical parameters of two different members of the exponential
family are related by the following equation:

$$\theta^i = \sum_{j=1}^r X_j^i \phi^j + \xi^i$$

- i means the ith member.
- j means the jth member.

The relationship between canonical parameters of two exponential
parametrization is same as the relationship between two cartesian coordinate
systems defined over the same plane. In the case of a cartesian system the X_j
is the rotation matrix, $\xi_j$ is the translation vector.

We can use the notion of canonical parameters/coordinates to define
geometric concepts. For example, we can define the notion of a straight line
in an exponential family. Remember each point is a member of an exponential
family, whose parameters are its coordinate. Hence a line is simply a subset
of the exponential family whose parameters/coordinates obey a certain
constraint. We say that a subset K of an exponential family is a line if its
image under some canonical coordinate set is a line in $R^r$ where $r$ is the
dimension of each coordinate. This property, that is being a line, is
independent of coordinates/parameters because a line is still a line under an
affine transformation. What is then an affine transformation ?

Affine spaces
--------------

Affine spaces are set of points. The points in this set has specific property
that when a special point, called origin is given, the other points behave
like vectors. The allowed behavior, that is the operations allowed by this
behavior define this set of points as an affine space.
Once the origin point is defined, the other points are tips of arrows based at
the chosen origin, and they are added or multiplied by scalars according to
parallelogram rules applied to their corresponding arrows.
For example $\vec{A(0,5)} + 5 = \vec{(5, 10)}$

Let's consider two points:

*********************
*
*   ^ "p+v"(1, 4)
*   |  
*   | "v"
*   |  
*   + "p"(1, 1)
*
*
*********************

Given a vector $v$ and a point $p$ like above. The tip of the vector $v$ can
be defined as $p + v$. From this point of view, the vector $v$ is an
translation operation which sends $p$ to $p+v$. Let's denote this operation
$+v$

Basically in affine spaces a choice of origin point sets up a one-to-one
correspondance between points and vectors. For example if $p$ is the point of
origin, and another point $q$ is defined. There is always a vector $v$ which
is defined by the equation $p = q + v$. This means that $q$ corresponds to the
vector $v$ which translates $p$ to $q$.

A general affine space is defined as a set $X$ and a vector space $V$, each 
$v \in V$ corresponds to a transformation $+v$ from $X$ to $X$, that is to
itself. This transformation is called translation by $v$. The translations
have to satisfy two rules:

- $(p + v) + w = p + (v + w)$ for any point $p$ and vectors $v$ and $w$
- given any $x,y \in X$ there must be a unique translation that moves one to
  another.

A characteristic of affine spaces are the presence of affine coordinates.
These coordinates are special coordinate systems. Let's give an example based
on the plane.

Let's consider two vectors $v_1, v_2$, that are linearly independent, that is,
$v_1 \cdot v_2$. Linearly independent implies the following:

$a_1 v_1 + a_2 v_2 + \dots + a_i v_i + \dots + a_n v_n = 0$ if and only if

$\forall a_i = 0$

No vector in the sequence can be represented as a linear combination of other
vectors.

These vectors form the basis of a space of vectors so that
every vector $v$ in this space can be expressed uniquely in the form 
$\theta_1 v_1 + \theta_2 v_2$. The numbers $(\theta_1, \theta_2)$ can be
regarded as the coordinates of $v$. If the length of vectors $v_1, v_2$ are of
unit length 1, and the angle between vectors is the right angle, and they are
in anticlockwise order, then such a coordinate system is a Cartesian
coordinate system. These kinds of coordinate systems are called affine
coordinates.

Here is how a general affine space works. We have a set of points, for example
in $R^n$. We also have a set of vectors $V$. We select one of the points to be
the origin, for example $o^n$. Now we select a set of vectors as basis from
$V$, for example: $v_1, v_2, \dots, v_n$.
The basis vectors are linearly independent. 
How do we express other points with respect to $o$ without knowing their
coordinates or having the notion of coordinates etc. Basically we express them
as constraints on the set $V$. That is each point is defined as a constraint
on the set $V$ for which there is a unique member of $V$ that satisfies it.

For example, we have the following

*********************
*
*      +-> "p+v=q"
*     /  
*    / "v"
*   /  
*  + "p"
*
*
*********************

How do we define $q$ without having the notion of distance ? If this is
happening in an affine space, then we must have an origin let say $p$ and a
set of vectors $V$ in which there exists a subset who consists of linearly
independent vectors, for example: $v_1, v_2, \dots, v_n$.

We can express $v$ as the following: $v = \theta_1(X, o)v_1 + \theta_2(X,
o)v_2 + \dots + \theta_n(X, o)v_n$ v where $\theta_i$ is function whose domain
is X, the set of points that constitute the affine space, and whose codomain
is V. It is a bijection. Basically it maps $v$ to $p$ using $o$, for example
as we saw in the $p = o + v$. The better way to express this given these
definitions would be $p = v(o)$. 

$\theta_i : V \to V$. $\theta$ is a linear function meaning that it has the
form: $\theta(v) = mv + b$

$ \theta_i(f(X)) $ where $f: X \to V$.

$ \theta(f(X)) = m f(X) + b$, where $m,b \in V$

Definition: Any collection of functions defined on X by using intermediary
*linear* functions of the form, $f(m) = f(x) + f(o)$ who are determined by the
origin point $o$, a bijection on $X \to V$, f, and a secondary point m, are
called an affine coordinate systems.

One can translate one affine coordinate system to another using a matrix and a
vector. The matrix is called change of basis matrix. The vector indicate the
coordinates for the other system. 
For example, let's say we have two affine coordinate systems, $\phi, \psi$.
Both of them are defined on the collection of points X. Let's say there is a
matrix $X_j^i$ and a vector $(u_1, \dots, u_r)$ both depend on $\phi$ and
$\psi$. The relationship that defines this dependence is the following:

$$ \phi(p) = \sum_{j=1}^{r} X_j^i \psi^j(p) + u^i $$

If two affine coordinate systems are related in this way, they are called
affinely related. The converse is also true, that is if there are two
collection of functions that are affinely related on a set of points, then
that set of points is also an affine space.


Let's say we have positive functions whose domain are positive densities.
For example, we have a positive function $f: [0,1] \to R^{+}$. We also have
positive densities $p_1: \Omega \to [0,1]$ and $p_2: \Omega \to [0,1]$.
Let's define the $f$ as $f = p_1 / p_2$. If we multiply $f$ by $p_1$, we would
obtain a set whose defined on the codomain of the function $f$. This set whose
defined on the codomain of the function $f$ belongs to a family of
transformations. These transformations apply to the set containing the
densities which are parametrised by positive functions like $f$.

In general multiplication of positive densities by positive functions gives a
family of transformations of the set of positive densities parametrised by
positive functions. Positive functions act like a vector space, that is they
help us to translate a positive density to another positive density.



p. 9, In order to give

<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js" charset="utf-8"></script><script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
